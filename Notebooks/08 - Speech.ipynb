{"cells":[{"cell_type":"markdown","source":["# Speech\n","\n","Increasingly, we expect to be able to communicate with artificial intelligence (AI) systems by talking to them, often with the expectation of a spoken response.\n","\n","![A robot speaking](./images/speech.jpg)\n","\n","*Speech recognition* (an AI system interpreting spoken language) and *speech synthesis* (an AI system generating a spoken response) are the key components of a speech-enabled AI solution.\n","\n","## Create a Cognitive Services resource\n","\n","To build software that can interpret audible speech and respond verbally, you can use the **Speech** cognitive service, which provides a simple way to transcribe spoken language into text and vice-versa.\n","\n","If you don't already have one, use the following steps to create a **Cognitive Services** resource in the Azure subscription:\n","\n","> **Note**: If you already have a Cognitive Services resource, just open its **Quick start** page in the Azure portal and copy its key and endpoint to the cell below. Otherwise, follow the steps below to create one.\n","\n","1. In another browser tab, open the Azure portal at https://portal.azure.com, sign in with the lab credentials.\n","2. Click the **&#65291;Create a resource** button, search for *Cognitive Services*, and create a **Cognitive Services** resource with the following settings:\n","    - **Subscription**: *Select the existing subscription where you are performing the lab*.\n","    - **Resource group**: *Select the existing resource group*.\n","    - **Region**: *Choose any available region or the region where the resource group is deployed*.\n","    - **Name**: *speech-uniqueID* , You can find the uniqueID value in the Lab Environment-> Environment details tab\n","    - **Pricing tier**: S0\n","    - **I confirm I have read and understood the notices**: Selected.\n","\n","3. Click on **Review+Create**. After the template has passed the validation click **create** to create the Cognitive Service.\n","4. Wait for deployment to complete. Then go to your cognitive services resource, and on the **Overview** page, click the link to manage the keys for the service. You will need the endpoint and keys to connect to your cognitive services resource from client applications.\n","\n","### Get the Key and Endpoint for your Cognitive Services resource\n","\n","To use your cognitive services resource, client applications need its  endpoint and authentication key:\n","\n","1. In the Azure portal, on the **Keys and Endpoint** page for your cognitive service resource, copy the **Key1** for your resource and paste it in the code below, replacing **YOUR_COG_KEY**.\n","2. Copy the **Endpoint** for your resource and and paste it in the code below, replacing **YOUR_COG_ENDPOINT**.\n","3. Copy the **Location** for your resource and and paste it in the code below, replacing **YOUR_COG_LOCATION**.\n","4. Run the code below by clicking the **Run cell** (&#9655;) button to the left of the cell."],"metadata":{}},{"cell_type":"code","source":["#Replace YOUR_COG_KEY and YOUR_COG_LOCATION with the cognitive service key and location values.\n","\n","cog_key = 'YOUR_COG_KEY'\n","cog_location = 'YOUR_COG_LOCATION'\n","\n","print('Ready to use cognitive services in {} using key {}'.format(cog_location, cog_key))"],"outputs":[],"execution_count":null,"metadata":{"gather":{"logged":1599695240794}}},{"cell_type":"markdown","source":["## Speech recognition\n","\n","Suppose you want to build a home automation system that accepts spoken instructions, such as \"turn the light on\" or \"turn the light off\". Your application needs to be able to take the audio-based input (your spoken instruction), and interpret it by transcribing it to text that it can then parse and analyze.\n","\n","Now you're ready to transcribe some speech. The input can be a microphone or an audio file. \n","\n","### Speech Recognition with a microphone\n","\n","Run the cell below and **immediately** say out loud **\"turn the light on\"**. The speech-to-text capabilities of the Speech service will transcribe the audio. The output should be your speech in text.\n"],"metadata":{}},{"cell_type":"code","source":["import os\n","import IPython\n","from azure.cognitiveservices.speech import SpeechConfig, SpeechRecognizer, AudioConfig\n","\n","# Configure speech recognizer\n","speech_config = SpeechConfig(cog_key, cog_location)\n","\n","# Have students say \"turn the light on\" \n","speech_recognizer = SpeechRecognizer(speech_config)\n","\n","# Use a one-time, synchronous call to transcribe the speech\n","speech = speech_recognizer.recognize_once()\n","\n","print(speech.text)\n"],"outputs":[],"execution_count":null,"metadata":{"gather":{"logged":1599695250434}}},{"source":["### (!) Check In\n","\n","Were you able to run the cell and translate your speech to text? If the above cell does not give a text output (example output: _Turn the light on._), try running the cell again and **immediately** say out loud \"turn the light on\".\n","\n","### Speech Recognition with an audio file\n","\n","If the cell above does not give a text output, your microphone may not be set up to accept input. Instead, run the cell below to see the Speech Recognition service in action with an audio file instead of microphone input. \n"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","from playsound import playsound\n","from azure.cognitiveservices.speech import SpeechConfig, SpeechRecognizer, AudioConfig\n","\n","# Get spoken command from audio file\n","file_name = 'light-on.wav'\n","audio_file = os.path.join('data', 'speech', file_name)\n","\n","# Configure speech recognizer\n","speech_config = SpeechConfig(cog_key, cog_location)\n","audio_config = AudioConfig(filename=audio_file) # Use file instead of default (microphone)\n","speech_recognizer = SpeechRecognizer(speech_config, audio_config)\n","\n","# Use a one-time, synchronous call to transcribe the speech\n","speech = speech_recognizer.recognize_once()\n","\n","# Play audio and show transcribed text\n","playsound(audio_file)\n","print(speech.text)"]},{"cell_type":"markdown","source":["## Speech synthesis\n","\n","So now you've seen how the Speech service can be used to transcribe speech into text; but what about the opposite? How can you convert text into speech?\n","\n","Well, let's assume your home automation system has interpreted a command to turn the light on. An appropriate response might be to acknowledge the command verbally (as well as actually performing the commanded task!)"],"metadata":{}},{"cell_type":"code","source":["import os\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","from azure.cognitiveservices.speech import SpeechConfig, SpeechSynthesizer, AudioConfig\n","%matplotlib inline\n","\n","# Get text to be spoken\n","response_text = 'Turning the light on.'\n","\n","# Configure speech synthesis\n","speech_config = SpeechConfig(cog_key, cog_location)\n","speech_synthesizer = SpeechSynthesizer(speech_config)\n","\n","# Transcribe text into speech\n","result = speech_synthesizer.speak_text(response_text)\n","\n","# Display an appropriate image \n","file_name = response_text + \"jpg\"\n","img = Image.open(os.path.join(\"data\", \"speech\", file_name))\n","plt.axis('off')\n","plt. imshow(img)"],"outputs":[],"execution_count":null,"metadata":{"gather":{"logged":1599695261170}}},{"cell_type":"markdown","source":["Try changing the **response_text** variable to *Turning the light off.* (including the period at the end) and run the cell again to hear the result.\n","\n","## Learn more\n","\n","You've seen a very simple example of using the Speech cognitive service in this notebook. You can learn more about [speech-to-text](https://docs.microsoft.com/azure/cognitive-services/speech-service/index-speech-to-text) and [text-to-speech](https://docs.microsoft.com/azure/cognitive-services/speech-service/index-text-to-speech) in the Speech service documentation."],"metadata":{}}],"metadata":{"language_info":{"name":"python","version":"3.8.5-final","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"name":"python3","display_name":"Python 3.8.5 32-bit","metadata":{"interpreter":{"hash":"177429bd1865e7f7a0dbecbac90518c0d9641b1102b2e6c0df4b82dc948b5cb2"}}},"kernel_info":{"name":"python3-azureml"},"nteract":{"version":"nteract-front-end@1.0.0"}},"nbformat":4,"nbformat_minor":2}