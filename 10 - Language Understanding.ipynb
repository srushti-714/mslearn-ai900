{"cells":[{"cell_type":"markdown","source":["# Language Understanding\n","\n","Increasingly, we expect computers to be able to use AI in order to understand spoken or typed commands in natural language. For example, you might want to implement a home automation system that enables you to control devices in your home by using voice commands such as \"switch on the light\" or \"put the fan on\", and have an AI-powered device understand the command and take appropriate action.\n","\n","![A robot listening](./images/language_understanding.jpg)\n","\n","## Create Authoring and Prediction Resources\n","\n","Microsoft cognitive services includes the Language Understanding service, which enables you to define *intents* that are applied to *entities* based on *utterances*. \n","\n","To use the Language Understanding service, you need two kinds of resource:\n","\n","- An *authoring* resource: used to define, train, and test the language model. This must be a **Language Understanding - Authoring** resource in your Azure subscription.\n","- A *prediction* resource: used to publish model and handle requests from client applications that use it. This can be either a **Language Understanding** or **Cognitive Services** resource in your Azure subscription.\n","\n","You can use either a **Language Understanding** or  **Cognitive Services** resource to *publish* a Language Understanding app, but you must create a separate **Language Understanding** resource for *authoring* the app.\n","\n","     > **Important**: Authoring resources must be created in one of three *regions* (Europe, Australia, or US). Models created in European or Australian authoring resources can only be deployed to prediction resources in Europe or Australia respectively; models created in US authoring resources can be deployed to prediction resources in any Azure location other than Europe and Australia. See the [authoring and publishing regions documentation](https://docs.microsoft.com/azure/cognitive-services/luis/luis-reference-regions) for details about matching authoring and prediction locations.\n","\n","1. In another browser tab, open the Azure portal at [https://portal.azure.com](https://portal.azure.com), sign in with your lab credentials.\n","2. Click **+ Create a resource**, and search for *Language Understanding*.\n","3. In the list of services, click **Language Understanding**.\n","4. In the **Language Understanding** blade, click **Create**.\n","5. In the **Create** blade, enter the following details and click **Create**\n","   - **Create option**: Both\n","   - **Name**: *lu-uniqueID* , You can find the uniqueID value in the Lab Environment-> Environment details tab.\n","   - **Subscription**: *Select the existing subscription where you are performing the lab*.\n","   - **Resource group**: *Select the existing resource group*.\n","   - **Authoring location**: *Select your preferred location*\n","   - **Authoring pricing tier**: F0\n","   - **Runtime location**: *Choose a location in the same region as your authoring location*\n","   - **Runtime pricing tier**: F0\n","\n","6. Wait for the resources to be created, and note that two Language Understanding resources are provisioned; one for authoring, and another for prediction. You can view these by navigating to the resource group where you created them.\n","\n","### Create a Language Understanding App\n","\n","To implement natural language understanding with Language Understanding, you create an app; and then add entities, intents, and utterances to define the commands you want the app to understand:\n","\n","1. In a new browser tab, open the Language Understanding portal for the authoring region where you created your authoring resource:\n","    - US: [https://www.luis.ai](https://www.luis.ai)\n","    - Europe: [https://eu.luis.ai](https://eu.luis.ai)\n","    - Australia: [https://au.luis.ai](https://au.luis.ai)\n","\n","2. Sign in using the lab credentials provided. If this is the first time you have signed into the Language Understanding portal, you may need to grant the app some permissions to access your account details. Then complete the *Welcome* steps by selecting the existing Language Understanding authoring resource you just created. \n","\n","3. Open the **Conversation Apps** page, and select your subscription and Language Understanding authoring resource. Then create a new app for conversation with the following settings:\n","   - **Name**: Home Automation\n","   - **Culture**: English (*if this option is not available, leave it blank*)\n","   - **Description**: Simple home automation\n","   - **Prediction resource**: *Your Language Understanding prediction resource*\n","\n","4. If a panel with tips for creating an effective Language Understanding app is displayed, close it.\n","\n","### Create an Entity\n","\n","An *entity* is a thing that your language model can identify and do something with. In this case, your Language Understanding app will be used to control various *devices* in the office, such as lights or fans; so you'll create a *device* entity that includes a list of the types of device that you want the app to work with. For each device type, you'll create a sublist that identifies the name of the device (for example *light*) and any synonyms that might be used to refer to this type of device (for example *lamp*).\n","\n","1. In the Language Understanding page for your app, in the pane on the left, click **Entities**. Then click **Create**, and create a new entity named **device**, select the **List** type, and click **Create**.\n","2. In the **List items** page, under **Normalized Values**, type **light**, then press ENTER.\n","3. After the **light** value has been added, under **Synonyms**, type **lamp** and press ENTER.\n","4. Add a second list item named **fan** with the synonym **AC**.\n","\n","### Create Intents\n","\n","An *intent* is an action you want to perform on one or more entities - for example, you might want to switch a light on, or turn a fan off. In this case, you'll define two intents: one to switch a device on, and another to switch a device off. For each intent, you'll specify sample *utterances* that indicate the kind of language used to indicate the intent.\n","\n","1. In the pane on the left, click **Intents**. Then click **Create**, and add an intent with the name **switch_on** and click **Done**.\n","2. Under the **Examples** heading and the **Example user input** subheading, type the utterance ***turn the light on*** and press **Enter** to submit this utterance to the list.\n","3. In the *turn the light on* utterance, click the word \"light\", and assign it to the **device** entity's **light** value.\n","4. Add a second utterance to the **switch_on** intent, with the phrase ***turn the fan on***. Then assign the word \"fan\" to the **device** entity's **fan** value.\n","5. In the pane on the left, click **Intents** and click **Create**, to add a second intent with the name **switch_off**.\n","6. In the **Utterances** page for the **switch_off** intent, add the utterance ***turn the light off*** and assign the word \"light\" to the **device** entity's **light** value.\n","7. Add a second utterance to the **switch_off** intent, with the phrase ***turn the fan off***. Then connect the word \"fan\" to the **device** entity's **fan** value.\n","\n","### Train and Test the Language Model\n","\n","Now you're ready to use the data you've provided in the form of entities, intents, and uterances to train the language model for your app.\n","\n","1. At the top of the Language Understanding page for your app, click **Train** to train the language model\n","2. When the model is trained, click **Test**, and use the Test pane to view the predicted intent for the following phrases:\n","    * *switch the light on*\n","    * *turn off the fan*\n","    * *turn the lamp off*\n","    * *switch on the AC*\n","3. Close the Test pane.\n","    \n","### Publish the Model and Configure Endpoints\n","\n","To use your trained model in a client application, you must publish it as an endpoint to which the client applications can send new utterances; from which intents and entitites will be predicted.\n","\n","1. At the top of the Language Understanding page for your app, click **Publish**. Then select **Production slot** and click **Done**.\n","\n","2. After the model has been published, at the top of the Language Understanding page for your app, click **Manage**. Then on the **Application Information** tab, note the **Application ID** for your app. Copy this and paste it in the code below to replace **YOUR_LU_APP_ID**.\n","\n","3. On the **Azure Resources** tab, note the **Primary key** and **Endpoint URL** for your prediction resource. Copy these and paste them into the code below, replacing **YOUR_LU_KEY** and **YOUR_LU_ENDPOINT**.\n","\n","4. Run the cell below by clicking its **Run cell** (&#9655;) button (to the left of the cell), and when prompted, enter the text *turn the light on*. The text is interpreted by your Language Understanding model and an appropriate image is displayed.\n"],"metadata":{}},{"cell_type":"code","source":["#Replace YOUR_LU_APP_ID,YOUR_LU_KEY and YOUR_LU_ENDPOINT \n","\n","from python_code import luis\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","import os\n","%matplotlib inline\n","\n","try:\n","    # Set up API configuration\n","    luis_app_id = 'YOUR_LU_APP_ID'\n","    luis_key = 'YOUR_LU_KEY'\n","    luis_endpoint = 'YOUR_LU_ENDPOINT'\n","\n","    # prompt for a command\n","    command = input('Please enter a command: \\n')\n","\n","    # get the predicted intent and entity (code in python_code.home_auto.py)\n","    action = luis.get_intent(luis_app_id, luis_key, luis_endpoint, command)\n","\n","    # display an appropriate image\n","    img_name = action + '.jpg'\n","    img = Image.open(os.path.join(\"data\", \"luis\" ,img_name))\n","    plt.axis('off')\n","    plt. imshow(img)\n","except Exception as ex:\n","    print(ex)"],"outputs":[],"execution_count":null,"metadata":{"tags":[],"gather":{"logged":1599696381331}}},{"cell_type":"markdown","source":["### (!) Check In \n","Did you run the cell above, then type the phrase *turn the light on* when prompted? \n","\n","Re-run the cell above, trying the following phrases:\n","\n","* *turn on the light*\n","* *put the lamp off*\n","* *switch the fan on*\n","* *switch the light on*\n","* *switch off the light*\n","* *turn off the fan*\n","* *switch the AC on*\n","\n","> **Note**: If you're curious about the code used to retrieve the intents and entitites from your Language Understanding app, look at the **luis.py** file in the **python_code** folder."],"metadata":{}},{"cell_type":"markdown","source":["## Add Voice Control\n","\n","So far, we've seen how analyze text; but increasingly AI systems enable humans to communicate with software services through speech recognition. To support this, the **Speech** cognitive service provides a simple way to transcribe spoken language into text.\n","\n","### Create a Cognitive Services Resource\n","\n","If you don't already have one, use the following steps to create a **Cognitive Services** resource in your Azure subscription:\n","\n","> **Note**: If you already have a Cognitive Services resource, just open its **Quick start** page in the Azure portal and copy its key and endpoint to the cell below. Otherwise, follow the steps below to create one.\n","\n","1. In another browser tab, open the Azure portal at [https://portal.azure.com](https://portal.azure.com), signin using the lab credentials.\n","2. Click the **&#65291;Create a resource** button, search for *Cognitive Services*, and create a **Cognitive Services** resource with the following settings:\n","    - **Subscription**: *Select the existing subscription where you are performing the lab*.\n","    - **Resource group**: *Select the existing resource group*.\n","    - **Region**: *Choose any available region or the region where the resource group is deployed*.\n","    - **Name**: *vc-uniqueID* , You can find the uniqueID value in the Lab Environment-> Environment details tab.\n","    - **Pricing tier**: S0\n","    - **I confirm I have read and understood the notices**: Selected.\n","3. Wait for deployment to complete. Then go to your cognitive services resource, and on the **Quick start** page, note the keys and endpoint. You will need these to connect to your cognitive services resource from client applications.\n","\n","### Get the Key and Endpoint for your Cognitive Services Resource\n","\n","To use your cognitive services resource, client applications need its  endpoint and authentication key:\n","\n","1. In the Azure portal, on the **Keys and Endpoint** page for your cognitive service resource, copy the **Key1** for your resource and paste it in the code below, replacing **YOUR_COG_KEY**.\n","2. Copy the **Endpoint** for your resource and and paste it in the code below, replacing **YOUR_COG_ENDPOINT**.\n","3. Copy the **Location** for your resource and paste it in the code below, replacing **YOUR_COG_REGION**    .\n","4. Run the code in the cell below. "],"metadata":{}},{"cell_type":"code","source":["#Replace YOUR_COG_KEY,YOUR_COG_ENDPOINT and YOUR_COG_REGION\n","cog_key = 'YOUR_COG_KEY'\n","cog_endpoint = 'YOUR_COG_ENDPOINT'\n","cog_region = 'YOUR_COG_REGION'\n","\n","print('Ready to use cognitive services in {} using key {}'.format(cog_region, cog_key))"],"outputs":[],"execution_count":null,"metadata":{"tags":[],"gather":{"logged":1599696409914}}},{"cell_type":"markdown","source":["Now run the cell below to transcribe speech from an audio file, and use it as a command for your Language Understanding app."],"metadata":{}},{"cell_type":"code","source":["import os\n","from python_code import luis\n","from azure.cognitiveservices.speech import SpeechConfig, SpeechRecognizer, AudioConfig\n","from playsound import playsound\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","%matplotlib inline\n","\n","try:   \n","\n","    # Get spoken command from audio file\n","    file_name = 'light-on.wav'\n","    audio_file = os.path.join('data', 'luis', file_name)\n","\n","    # Configure speech recognizer\n","    speech_config = SpeechConfig(cog_key, cog_region)\n","    audio_config = AudioConfig(filename=audio_file) # Use file instead of default (microphone)\n","    speech_recognizer = SpeechRecognizer(speech_config, audio_config)\n","\n","    # Use a one-time, synchronous call to transcribe the speech\n","    speech = speech_recognizer.recognize_once()\n","\n","    # Get the predicted intent and entity (code in python_code.home_auto.py)\n","    action = luis.get_intent(luis_app_id, luis_key, luis_endpoint, speech.text)\n","\n","    # Get the appropriate image\n","    img_name = action + '.jpg'\n","\n","    # Play audio \n","    playsound(audio_file)\n","\n","    # Display image \n","    img = Image.open(os.path.join(\"data\", \"luis\" ,img_name))\n","    plt.axis('off')\n","    plt. imshow(img)\n","\n","except Exception as ex:\n","    print(ex)"],"outputs":[],"execution_count":null,"metadata":{"tags":[],"gather":{"logged":1599696420498}}},{"cell_type":"markdown","source":["Try modifying the cell above to use the **light-off.wav** audio file.\n","\n","## Learn More\n","\n","Learn more about Language Understanding in the [service documentation](https://docs.microsoft.com/azure/cognitive-services/luis/)"],"metadata":{}}],"metadata":{"language_info":{"name":"python","version":"3.6.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"name":"python3-azureml","language":"python","display_name":"Python 3.6 - AzureML"},"kernel_info":{"name":"python3-azureml"},"nteract":{"version":"nteract-front-end@1.0.0"}},"nbformat":4,"nbformat_minor":2}